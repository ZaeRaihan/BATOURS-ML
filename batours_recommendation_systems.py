# -*- coding: utf-8 -*-
"""BATOURS Recommendation Systems.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H9FbkwiPNGqn_5n5ggPhLBJ2Ck0eDTZf
"""

# import libraries that will be used
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers

# mount drive
from google.colab import drive
drive.mount('/content/drive')

# upload datasets into pandas dataframe
ratings = pd.read_csv('/content/drive/MyDrive/BATOURS/tourism_rating.csv', sep=',', error_bad_lines=False, encoding="latin-1")
places = pd.read_csv('/content/drive/MyDrive/BATOURS/tourism_with_id.csv', sep=',', error_bad_lines=False, encoding="latin-1")

# see the first 5 data from the datasets

places.head()

ratings.head()

# merge the places and ratings dataframe, and then choose only when City is Bandung
merged_df = pd.merge(ratings, places, on='Place_Id')
merged_df = merged_df[merged_df['City']=='Bandung']
merged_df.head()

# drop the unnecessary columns
columns_drop = ['Place_Name', 'Description', 'Category', 'City', 'Price', 'Rating', 'Time_Minutes', 'Coordinate', 'Lat', 'Long', 'Unnamed: 11', 'Unnamed: 12']
merged_df.drop(columns_drop, axis=1, inplace=True)
merged_df.head()

# make list for unique user ID
user_ids = merged_df["User_Id"].unique().tolist()

# encode every unique user ID to a value
user_encoder = {x: i for i, x in enumerate(user_ids)}

# decode the encoded value to be the original user ID
user_decoder = {i: x for i, x in enumerate(user_ids)}

# make list for unique place ID
place_ids = merged_df["Place_Id"].unique().tolist()

# encode every unique place ID to a value
place_encoder = {x: i for i, x in enumerate(place_ids)}

# decode the encoded value to be the original place ID
place_decoder = {i: x for i, x in enumerate(place_ids)}

# make new column that store the encoded value of user ID and place ID
merged_df["user"] = merged_df["User_Id"].map(user_encoder)
merged_df["place"] = merged_df["Place_Id"].map(place_encoder)

# make new column to ensures the rating are represented as 32-bit floating-point numbers
merged_df["rating"] = merged_df["Place_Ratings"].values.astype(np.float32)

num_users = len(user_encoder)
num_places = len(place_decoder)
lowest_rating = min(merged_df["rating"])
highest_rating = max(merged_df["rating"])

print(
    "Number of users: {}, Number of Places: {}, Lowest rating: {}, Highest rating: {}".format(
        num_users, num_places, lowest_rating, highest_rating
    )
)

# shuffle all rows with random seed 42
merged_df = merged_df.sample(frac=1, random_state=42)

# store the user and place combination features to x
x = merged_df[["user", "place"]].values

# store the scaled rating values label using min-max normalization to y
y = merged_df["rating"].apply(lambda x: (x - lowest_rating) / (highest_rating - lowest_rating)).values

# split the data to training and validation sets
train_indices = int(0.7 * merged_df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:],
)

EMBEDDING_SIZE = 50

# recommender system model from keras.Model
class RecommenderNet(keras.Model):
    # create embedding layers for users and places
    def __init__(self, num_users, num_places, embedding_size, **kwargs):
        super().__init__(**kwargs)
        # parameters
        self.num_users = num_users
        self.num_places = num_places
        self.embedding_size = embedding_size
        # user embedding layer
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        # user bias layer
        self.user_bias = layers.Embedding(num_users, 1)
        # placer embedding layer
        self.place_embedding = layers.Embedding(
            num_places,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        # place bias layer
        self.place_bias = layers.Embedding(num_places, 1)
    # forward pass
    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        place_vector = self.place_embedding(inputs[:, 1])
        place_bias = self.place_bias(inputs[:, 1])
        # dot product between user vector and place vector
        dot_user_place = tf.tensordot(user_vector, place_vector, 2)
        # Add all the components, including bias
        x = dot_user_place + user_bias + place_bias
        # Use sigmoid activation function to forces the rating between 0 and 1
        return tf.nn.sigmoid(x)

# recommender system model
model = RecommenderNet(num_users, num_places, EMBEDDING_SIZE)
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
)

# train model
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=32,
    epochs=10,
    verbose=1,
    validation_data=(x_val, y_val),
)

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

# selects random user ID
user_id = merged_df.User_Id.sample(1).iloc[0]

# places already visited by the selected user
places_visited = merged_df[merged_df.User_Id == user_id]

# places not visited yet by the selected user
places_not_visited = places[
    ~places["Place_Id"].isin(places_visited.Place_Id.values)
]["Place_Id"]

# ensures only places which embedding is available are considered for recommendation
places_not_visited = list(
    set(places_not_visited).intersection(set(place_encoder.keys()))
)
places_not_visited = [[place_encoder.get(x)] for x in places_not_visited]

# encoded value of the selected user
user_encoded_value = user_encoder.get(user_id)

# user-place combination array
user_place_array = np.hstack(
    ([[user_encoded_value]] * len(places_not_visited), places_not_visited)
)

# predict the rating
ratings = model.predict(user_place_array).flatten()

# sort the rating highest to lowest
top_ratings_indices = ratings.argsort()[-10:][::-1]

# list of recommended place from encoded value
recommended_place_ids = [
    place_decoder.get(places_not_visited[x][0]) for x in top_ratings_indices
]

print("Top 10 place recommendations for user-{}:\n".format(user_id))
recommended_places = places[places["Place_Id"].isin(recommended_place_ids)]
i = 1
for row in recommended_places.itertuples():
    print(i, row.Place_Name, "-", row.Category)
    i += 1